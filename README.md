# MRI_tumor_detection
This project delivers a production-ready deep-learning pipeline that automatically classifies axial brain-MRI slices into four diagnostic categories—glioma, meningioma, pituitary tumour and healthy tissue—offering radiologists an AI-assisted triage tool while showcasing modern computer-vision and MLOps skills. We start with the public Kaggle Brain-Tumor MRI dataset (≈ 6.4 k PNG/JPEG images, predominately 512 × 512 px) and create 80 / 10 / 10 train-validation-test splits using split-folders, preserving class balance. Each image is resized to 256 px (largest edge) then centre-cropped to 224 × 224, converted to RGB and normalised with ImageNet statistics; augmentations (random flips, ±15° rotations, brightness/contrast jitter and slight Gaussian noise) are applied on-the-fly via torchvision.transforms to combat overfitting and mimic scanner variability. The backbone is an ImageNet-pretrained EfficientNet-B0 implemented in PyTorch on Apple-silicon GPUs (MPS backend); we replace its classifier with a four-unit fully connected head, freeze the lower 70 % of layers, then fine-tune the upper blocks using Adam (lr = 1e-4 with cosine decay), label-smoothing cross-entropy and optional class-weighted focal loss (γ = 2) to improve minority-tumour recall. Training is monitored with TensorBoard and MLflow, recording per-epoch accuracy, loss, precision-recall and confusion-matrix artefacts; early stopping halts training when validation loss plateaus for five epochs, typically yielding ≥ 95 % overall accuracy and > 0.90 F1 for every class. Explainability is provided via Grad-CAM: heat-maps are generated for validation images and saved alongside predictions, visually highlighting tumour regions that drive the network’s decision—crucial for clinician trust. Once the best checkpoint is saved (best_model.pth plus class-index JSON), we expose an interactive Streamlit app: users drag-and-drop an MRI slice to receive the predicted label, confidence and a Grad-CAM overlay in < 0.4 s. For deployment, the inference script and model are containerised with Docker, wrapped by a FastAPI REST endpoint, and provisioned via a lightweight GitHub-Actions CI/CD pipeline to AWS EC2 (or GCP Cloud Run), demonstrating full MLOps competence from data ingestion through model serving and monitoring.
